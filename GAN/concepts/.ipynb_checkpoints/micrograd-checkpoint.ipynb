{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP\n",
    "\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \n",
    "    # load image\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "    one_hot = lambda x, k: np.array(x[:,None] == np.arange(k)[None, :], dtype=int)\n",
    "\n",
    "    # rescale the data, use the traditional train/test split\n",
    "    X_train, X_test = X[:60000], X[60000:]\n",
    "    y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0\n",
    "\n",
    "    y_train = np.array([int(k) for k in y_train])\n",
    "    y_test = np.array([int(k) for k in y_test])\n",
    "\n",
    "    y_train = one_hot(y_train, 10)\n",
    "    y_test = one_hot(y_test, 10)\n",
    "\n",
    "    N_data = X_train.shape[0]\n",
    "\n",
    "    return N_data, X_train, y_train, X_test, y_test\n",
    "\n",
    "def save_images(images, filename, **kwargs):\n",
    "    fig = plt.figure(1)\n",
    "    fig.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plot_images(images, ax, **kwargs)\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.patch.set_visible(False)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyper parameters\n",
    "noise_dim = 10\n",
    "gen_layer_sizes = [noise_dim, 200, 784]\n",
    "dsc_layer_sizes = [784, 200, 1]\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "step_size_max = 0.01\n",
    "step_size_min = 0.01\n",
    "\n",
    "print('Loading training data')\n",
    "N_data, X_train, y_train, X_test, y_test = load_mnist()\n",
    "N, train_images, _, test_images, _ = N_data, X_train, y_train, X_test, y_test #load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define geneerator, discriminator, and objective ###\n",
    "\n",
    "def relu(x):       return np.maximum(0, x)\n",
    "def sigmoid(x):    return 0.5 * (np.tanh(x) + 1.0)\n",
    "def logsigmoid(x): return x - np.logaddexp(0, x)\n",
    "\n",
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples,\n",
    "       one for each layer in the net.\"\"\"\n",
    "    return [(scale * rs.randn(m, n),   # weight matrix\n",
    "             scale * rs.randn(n))      # bias vector\n",
    "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "def batch_normalize(activations):\n",
    "    mbmean = np.mean(activations, axis=0, keepdims=True)\n",
    "    return (activations - mbmean) / (np.std(activations, axis=0, keepdims=True) + 1)\n",
    "\n",
    "def neural_net_predict(params, inputs):\n",
    "    \"\"\"Params is a list of (weights, bias) tuples.\n",
    "       inputs is an (N x D) matrix.\"\"\"\n",
    "    inpW, inpb = params[0]\n",
    "    inputs = relu(np.dot(inputs, inpW) + inpb)\n",
    "    for W, b in params[1:-1]:\n",
    "        outputs = batch_normalize(np.dot(inputs, W) + b)\n",
    "        inputs = relu(outputs)\n",
    "    outW, outb = params[-1]\n",
    "    outputs = np.dot(inputs, outW) + outb\n",
    "    return outputs\n",
    "\n",
    "def generate_from_noise(gen_params, num_samples, noise_dim, rs):\n",
    "    noise = rs.rand(num_samples, noise_dim)\n",
    "    samples = neural_net_predict(gen_params, noise)\n",
    "    return sigmoid(samples)\n",
    "\n",
    "def gan_objective(gen_params, dsc_params, real_data, num_samples, noise_dim, rs):\n",
    "    fake_data = generate_from_noise(gen_params, num_samples, noise_dim, rs)\n",
    "    logprobs_fake = logsigmoid(neural_net_predict(dsc_params, fake_data))\n",
    "    logprobs_real = logsigmoid(neural_net_predict(dsc_params, real_data))\n",
    "    return np.mean(logprobs_real) - np.mean(logprobs_fake)\n",
    "\n",
    "\n",
    "### Define minimax version of adam optimizer ###\n",
    "\n",
    "def adam_minimax(grad_both, init_params_max, init_params_min, callback=None, num_iters=100,\n",
    "         step_size_max=0.001, step_size_min=0.001, b1=0.9, b2=0.999, eps=10**-8):\n",
    "    \"\"\"Adam modified to do minimiax optimization, for instance to help with\n",
    "    training generative adversarial networks.\"\"\"\n",
    "\n",
    "    x_max, unflatten_max = flatten(init_params_max)\n",
    "    x_min, unflatten_min = flatten(init_params_min)\n",
    "\n",
    "    m_max = np.zeros(len(x_max))\n",
    "    v_max = np.zeros(len(x_max))\n",
    "    m_min = np.zeros(len(x_min))\n",
    "    v_min = np.zeros(len(x_min))\n",
    "    for i in range(num_iters):\n",
    "        g_max_uf, g_min_uf = grad_both(unflatten_max(x_max),\n",
    "                                       unflatten_min(x_min), i)\n",
    "        g_max, _ = flatten(g_max_uf)\n",
    "        g_min, _ = flatten(g_min_uf)\n",
    "\n",
    "        if callback: callback(unflatten_max(x_max), unflatten_min(x_min), i,\n",
    "                              unflatten_max(g_max), unflatten_min(g_min))\n",
    "\n",
    "        m_max = (1 - b1) * g_max      + b1 * m_max  # First  moment estimate.\n",
    "        v_max = (1 - b2) * (g_max**2) + b2 * v_max  # Second moment estimate.\n",
    "        mhat_max = m_max / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat_max = v_max / (1 - b2**(i + 1))\n",
    "        x_max = x_max + step_size_max * mhat_max / (np.sqrt(vhat_max) + eps)\n",
    "\n",
    "        m_min = (1 - b1) * g_min      + b1 * m_min  # First  moment estimate.\n",
    "        v_min = (1 - b2) * (g_min**2) + b2 * v_min  # Second moment estimate.\n",
    "        mhat_min = m_min / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat_min = v_min / (1 - b2**(i + 1))\n",
    "        x_min = x_min - step_size_min * mhat_min / (np.sqrt(vhat_min) + eps)\n",
    "    return unflatten_max(x_max), unflatten_min(x_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model hyper-parameters\n",
    "noise_dim = 10\n",
    "gen_layer_sizes = [noise_dim, 200, 784]\n",
    "dsc_layer_sizes = [784, 200, 1]\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "step_size_max = 0.01\n",
    "step_size_min = 0.01\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "\n",
    "init_gen_params = init_random_params(param_scale, gen_layer_sizes)\n",
    "init_dsc_params = init_random_params(param_scale, dsc_layer_sizes)\n",
    "\n",
    "num_batches = int(np.ceil(len(train_images) / batch_size))\n",
    "def batch_indices(iter):\n",
    "    idx = iter % num_batches\n",
    "    return slice(idx * batch_size, (idx+1) * batch_size)\n",
    "\n",
    "# Define training objective\n",
    "seed = npr.RandomState(0)\n",
    "def objective(gen_params, dsc_params, iter):\n",
    "    idx = batch_indices(iter)\n",
    "    return gan_objective(gen_params, dsc_params, train_images[idx],\n",
    "                         batch_size, noise_dim, seed)\n",
    "\n",
    "# Get gradients of objective using autograd.\n",
    "both_objective_grad = grad(objective, argnum=(0, 1))\n",
    "\n",
    "print(\"     Epoch     |    Objective  |       Fake probability | Real Probability  \")\n",
    "def print_perf(gen_params, dsc_params, iter, gen_gradient, dsc_gradient):\n",
    "    if iter % 10 == 0:\n",
    "        ability = np.mean(objective(gen_params, dsc_params, iter))\n",
    "        fake_data = generate_from_noise(gen_params, 20, noise_dim, seed)\n",
    "        real_data = train_images[batch_indices(iter)]\n",
    "        probs_fake = np.mean(sigmoid(neural_net_predict(dsc_params, fake_data)))\n",
    "        probs_real = np.mean(sigmoid(neural_net_predict(dsc_params, real_data)))\n",
    "        print(\"{:15}|{:20}|{:20}|{:20}\".format(iter//num_batches, ability, probs_fake, probs_real))\n",
    "        save_images(fake_data, 'gan_samples.png', vmin=0, vmax=1)\n",
    "\n",
    "# The optimizers provided can optimize lists, tuples, or dicts of parameters.\n",
    "optimized_params = adam_minimax(both_objective_grad,\n",
    "                                init_gen_params, init_dsc_params,\n",
    "                                step_size_max=step_size_max, step_size_min=step_size_min,\n",
    "                                num_iters=num_epochs * num_batches, callback=print_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "from micrograd.engine import Value\n",
    "import math\n",
    "\n",
    "alpha = 0.01\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-8\n",
    "def func(x):\n",
    "\treturn x*x -4*x + 4\n",
    "def grad_func(x):\n",
    "\treturn 2*x - 4\n",
    "theta_0 = Value(0)\n",
    "m_t = 0 \n",
    "v_t = 0 \n",
    "t = 0\n",
    "\n",
    "while (1):\n",
    "\tt+=1\n",
    "#\tg_t = grad_func(theta_0)\t\t#computes the gradient of the stochastic function\n",
    "\tf_pass = func(theta_0)\n",
    "\tf_pass.backward()\n",
    "\tg_t = theta_0.grad\n",
    "\tm_t = beta_1*m_t + (1-beta_1)*g_t\t#updates the moving averages of the gradient\n",
    "\tv_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)\t#updates the moving averages of the squared gradient\n",
    "\tm_cap = m_t/(1-(beta_1**t))\t\t#calculates the bias-corrected estimates\n",
    "\tv_cap = v_t/(1-(beta_2**t))\t\t#calculates the bias-corrected estimates\n",
    "\ttheta_0_prev = theta_0\t\t\t\t\t\t\t\t\n",
    "\ttheta_0 = theta_0 - (alpha*m_cap)/(math.sqrt(v_cap)+epsilon)\t#updates the parameters\n",
    "\tif(theta_0.data == theta_0_prev.data):\t\t#checks if it is converged or not\n",
    "\t\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data, X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_layer_sizes = [noise_dim, 200, 784]\n",
    "dsc_layer_sizes = [784, 200, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = MLP(2, gen_layer_sizes) # 2-layer neural network\n",
    "dsc_model = MLP(2, dsc_layer_sizes) # 2-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(2, [784, 200, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [list(map(Value, xrow)) for xrow in X_train[:2]]\n",
    "\n",
    "# forward the model to get scores\n",
    "scores = list(map(model, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "v = scores[0][2].relu().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    \n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Cross entropy loss\n",
    "    losses = [scorei[yi].relu().log() for yi, scorei in zip(yb, scores)]\n",
    "    total_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = np.random.randn(32, noise_dim)[:2]\n",
    "input_list = [list(map(Value, xrow)) for xrow in input_list]\n",
    "temp = list(map(gen_model, input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss():\n",
    "    \n",
    "    \n",
    "def discriminator_loss():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsc loss\n",
    "\n",
    "fake_data = generate_from_noise(gen_params, num_samples, noise_dim, rs)\n",
    "logprobs_fake = logsigmoid(neural_net_predict(dsc_params, fake_data))\n",
    "logprobs_real = logsigmoid(neural_net_predict(dsc_params, real_data))\n",
    "logsigmoid()\n",
    "\n",
    "\n",
    "# gen loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
